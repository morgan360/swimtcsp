{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d33a70",
   "metadata": {},
   "source": [
    "# Swim TCSP Import CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c6d0d0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:39.755279Z",
     "end_time": "2023-12-27T13:34:39.839185Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:39.762446Z",
     "end_time": "2023-12-27T13:34:39.893159Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "9b3ac9b0",
   "metadata": {},
   "source": [
    "## Connection and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70084f2d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:39.779918Z",
     "end_time": "2023-12-27T13:34:39.898847Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pymysql\n",
    "\n",
    "def connect_to_tcsp():\n",
    "    connection_details = {\n",
    "        \"host\": 'tcsp.ie',\n",
    "        \"port\": 3306,\n",
    "        \"user\": 't567715',\n",
    "        \"password\": '0bjs8Pz55Q',\n",
    "        \"database\": 't567715_wp_tcsp',\n",
    "        \"charset\": 'utf8mb4',\n",
    "        \"cursorclass\": pymysql.cursors.DictCursor\n",
    "    }\n",
    "    try:\n",
    "        connection = pymysql.connect(**connection_details)\n",
    "        return connection\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Error connecting to MySQL Database: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f07898",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:39.872384Z",
     "end_time": "2023-12-27T13:34:40.117063Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_mor_events(connection):\n",
    "    if connection is None:\n",
    "        return []\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT id, event FROM mor_events\")\n",
    "            return cursor.fetchall()\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_data_to_csv(data, filename):\n",
    "    if data:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    conx = connect_to_tcsp()\n",
    "    if conx is not None:\n",
    "        data = fetch_mor_events(conx)\n",
    "        save_data_to_csv(data, 'public_swim_catagories.csv')\n",
    "finally:\n",
    "    if conx:\n",
    "        conx.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get field names\n",
    "This is used to get a list of field names for the processes below"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'customer_id', 'session_id', 'session_date', 'wc_order_id', 'order_type', 'booking_date', 'num_party', 'num_adults', 'num_children', 'num_senior', 'num_under3', 'num_guests', 'covid_check1', 'covid_check2', 'covid_check3', 'paid', 'attended', 'notes', 'adult_price', 'child_price', 'senior_price', 'under3_price', 'event']\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "def get_table_field_names(connection, table_name):\n",
    "    if connection is None:\n",
    "        return []\n",
    "    try:\n",
    "        with connection.cursor(pymysql.cursors.DictCursor) as cursor:  # Use DictCursor\n",
    "            cursor.execute(f\"DESCRIBE {table_name}\")\n",
    "            # Extracting only the column names using DictCursor\n",
    "            return [row[\"Field\"] for row in cursor.fetchall()]  # Adjusted to access 'Field' key\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    conx = connect_to_tcsp()  # Replace with your actual connection setup function\n",
    "    if conx is not None:\n",
    "        field_names = get_table_field_names(conx, 'mor_generic_bookings')\n",
    "        print(field_names)\n",
    "finally:\n",
    "    if 'conx' in locals() and conx:\n",
    "        conx.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:40.120466Z",
     "end_time": "2023-12-27T13:34:40.354676Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## More Generic (Supply sql and File names pairs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import csv\n",
    "import pymysql\n",
    "\n",
    "def execute_query(connection, query):\n",
    "    if connection is None:\n",
    "        return []\n",
    "    try:\n",
    "        with connection.cursor(pymysql.cursors.DictCursor) as cursor:  # Use DictCursor\n",
    "            cursor.execute(query)\n",
    "            return cursor.fetchall()\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_data_to_csv(data, filename):\n",
    "    if data:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "\n",
    "# Queries and corresponding CSV file names\n",
    "#Public Swims\n",
    "column_names_public_swims = ['id', 'day_id', 'event_id', 'num_places', 'time_start', 'time_end', 'notes', 'active']\n",
    "query_public_swims = f\"SELECT {', '.join(column_names_public_swims)} FROM mor_sessions_generic\"\n",
    "# Orders\n",
    "column_names_orders = ['wc_order_id as id', 'customer_id as user_id', 'session_id as product_id', 'session_date as booking ', 'wc_order_id as stripe_id']\n",
    "query_orders = f\"SELECT {', '.join(column_names_orders)} FROM mor_generic_bookings WHERE booking_date > '2023-11-25 \" \\\n",
    "               f\"12:16:17' AND wc_order_id IS NOT NULL ;\"\n",
    "# Order Items\n",
    "column_names_order_items = ['wc_order_id as id', 'num_adults', 'num_children', 'num_senior', 'num_under3']\n",
    "\n",
    "query_order_items= f\"SELECT {', '.join(column_names_order_items)} FROM mor_generic_bookings WHERE booking_date > \" \\\n",
    "                  f\"'2023-11-25 \" \\\n",
    "               f\"12:16:17' AND wc_order_id IS NOT NULL ;\"\n",
    "\n",
    "#Data for query\n",
    "query_file_pairs = [\n",
    "    # public_swim_categories\n",
    "    (\"SELECT id, event FROM mor_events\", \"public_swim_categories.csv\"),\n",
    "    # terms\n",
    "    (\"SELECT start_date, finish_date, COALESCE(rebook_start, '2000-01-01') AS rebook_start, COALESCE(booking_switch_date, '2000-01-01') AS booking_switch_date, COALESCE(assesments_complete, '2000-01-01') AS assesments_complete FROM mor_terms\", \"terms.csv\"),\n",
    "    # Public Swims\n",
    "    (query_public_swims, \"public_swims.csv\"),\n",
    "    # Public Swims Orders\n",
    "    (query_orders, \"public_swims_orders.csv\"),\n",
    "    # Public Swims Order items\n",
    "    (query_order_items, \"public_swims_order_items.csv\"),\n",
    "]\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    conx = connect_to_tcsp()  # Replace with your actual connection setup function\n",
    "    if conx is not None:\n",
    "        for query, filename in query_file_pairs:\n",
    "            data = execute_query(conx, query)\n",
    "            save_data_to_csv(data, filename)\n",
    "finally:\n",
    "    if 'conx' in locals() and conx:\n",
    "        conx.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T14:19:35.491528Z",
     "end_time": "2023-12-27T14:19:35.939219Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract User Data\n",
    "The data is all extracted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import csv\n",
    "import pymysql\n",
    "\n",
    "def connect_to_tcsp():\n",
    "    connection_details = {\n",
    "        \"host\": 'tcsp.ie',\n",
    "        \"port\": 3306,\n",
    "        \"user\": 't567715',\n",
    "        \"password\": '0bjs8Pz55Q',\n",
    "        \"database\": 't567715_wp_tcsp',\n",
    "        \"charset\": 'utf8mb4',\n",
    "        \"cursorclass\": pymysql.cursors.DictCursor\n",
    "    }\n",
    "    try:\n",
    "        connection = pymysql.connect(**connection_details)\n",
    "        return connection\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Error connecting to MySQL Database: {e}\")\n",
    "        return None\n",
    "\n",
    "# SQL query with WHERE clause to filter users with ID greater than 13149\n",
    "query = '''\n",
    "SELECT\n",
    "    u1.ID AS user_id,\n",
    "    u1.user_email AS user_email,\n",
    "    u1.user_login AS username,\n",
    "    m4.meta_value AS mobile_phone,\n",
    "    m7.meta_value AS user_phone,\n",
    "    m5.meta_value AS Role,\n",
    "    m8.meta_value AS notes,\n",
    "    m9.meta_value AS other_phone,\n",
    "    m10.meta_value AS first_name,\n",
    "    m11.meta_value AS last_name\n",
    "FROM wpmor_users u1\n",
    "LEFT JOIN wpmor_usermeta m4 ON m4.user_id = u1.ID AND m4.meta_key = 'mobile'\n",
    "LEFT JOIN wpmor_usermeta m5 ON m5.user_id = u1.ID AND m5.meta_key = 'wpmor_capabilities'\n",
    "LEFT JOIN wpmor_usermeta m7 ON m7.user_id = u1.ID AND m7.meta_key = 'user_phone'\n",
    "LEFT JOIN wpmor_usermeta m8 ON m8.user_id = u1.ID AND m8.meta_key = 'description'\n",
    "LEFT JOIN wpmor_usermeta m9 ON m9.user_id = u1.ID AND m9.meta_key = 'billing_phone'\n",
    "LEFT JOIN wpmor_usermeta m10 ON m10.user_id = u1.ID AND m10.meta_key = 'first_name'\n",
    "LEFT JOIN wpmor_usermeta m11 ON m11.user_id = u1.ID AND m11.meta_key = 'last_name'\n",
    "WHERE u1.ID > 13149;\n",
    "'''\n",
    "\n",
    "connection = connect_to_tcsp()\n",
    "if connection:\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "\n",
    "            with open('users.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                # Write the header\n",
    "                writer.writerow([i[0] for i in cursor.description])\n",
    "                # Write the data\n",
    "                for row in cursor:\n",
    "                    writer.writerow(row.values())\n",
    "    finally:\n",
    "        connection.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:40.845278Z",
     "end_time": "2023-12-27T13:34:41.312900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## New Users Only"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# This is to get new users from a certain user id\n",
    "\n",
    "import csv\n",
    "import pymysql\n",
    "\n",
    "def connect_to_tcsp():\n",
    "    connection_details = {\n",
    "        \"host\": 'tcsp.ie',\n",
    "        \"port\": 3306,\n",
    "        \"user\": 't567715',\n",
    "        \"password\": '0bjs8Pz55Q',\n",
    "        \"database\": 't567715_wp_tcsp',\n",
    "        \"charset\": 'utf8mb4',\n",
    "        \"cursorclass\": pymysql.cursors.DictCursor\n",
    "    }\n",
    "    try:\n",
    "        connection = pymysql.connect(**connection_details)\n",
    "        return connection\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Error connecting to MySQL Database: {e}\")\n",
    "        return None\n",
    "\n",
    "# SQL query with WHERE clause to filter users with ID greater than 13149\n",
    "query = '''\n",
    "SELECT\n",
    "    u1.ID AS user_id,\n",
    "    u1.user_email AS user_email,\n",
    "    u1.user_login AS username,\n",
    "    m4.meta_value AS mobile_phone,\n",
    "    m7.meta_value AS user_phone,\n",
    "    m5.meta_value AS Role,\n",
    "    m8.meta_value AS notes,\n",
    "    m9.meta_value AS other_phone,\n",
    "    m10.meta_value AS first_name,\n",
    "    m11.meta_value AS last_name\n",
    "FROM wpmor_users u1\n",
    "LEFT JOIN wpmor_usermeta m4 ON m4.user_id = u1.ID AND m4.meta_key = 'mobile'\n",
    "LEFT JOIN wpmor_usermeta m5 ON m5.user_id = u1.ID AND m5.meta_key = 'wpmor_capabilities'\n",
    "LEFT JOIN wpmor_usermeta m7 ON m7.user_id = u1.ID AND m7.meta_key = 'user_phone'\n",
    "LEFT JOIN wpmor_usermeta m8 ON m8.user_id = u1.ID AND m8.meta_key = 'description'\n",
    "LEFT JOIN wpmor_usermeta m9 ON m9.user_id = u1.ID AND m9.meta_key = 'billing_phone'\n",
    "LEFT JOIN wpmor_usermeta m10 ON m10.user_id = u1.ID AND m10.meta_key = 'first_name'\n",
    "LEFT JOIN wpmor_usermeta m11 ON m11.user_id = u1.ID AND m11.meta_key = 'last_name'\n",
    "WHERE u1.ID >= 13146;\n",
    "'''\n",
    "\n",
    "connection = connect_to_tcsp()\n",
    "if connection:\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "\n",
    "            with open('new_users.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                # Write the header\n",
    "                writer.writerow([i[0] for i in cursor.description])\n",
    "                # Write the data\n",
    "                for row in cursor:\n",
    "                    writer.writerow(row.values())\n",
    "    finally:\n",
    "        connection.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T14:18:36.232672Z",
     "end_time": "2023-12-27T14:18:36.487730Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# pip install phpserialize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:41.889505Z",
     "end_time": "2023-12-27T13:34:41.899436Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This program takes out the roles from the user data which is exported to users.csv from the database(TCSP) and creats a new file roles_output with the roles deserilised ro comma separated groups."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Outputs new_users to new roles\n",
    "import csv\n",
    "import phpserialize\n",
    "\n",
    "input_file = 'new_users.csv'  # Update the path if needed\n",
    "output_file = 'new_roles_output.csv'\n",
    "\n",
    "def deserialize_php(serialized_php):\n",
    "    try:\n",
    "        return phpserialize.loads(serialized_php.encode(), decode_strings=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deserializing: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = [name for name in reader.fieldnames if name != 'Role'] + ['groups']\n",
    "\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            processed_emails = set()  # Set to store processed emails\n",
    "\n",
    "            for row in reader:\n",
    "                email = row.get('email', '').strip().lower()\n",
    "                if email in processed_emails:\n",
    "                    continue  # Skip this row as email is already processed\n",
    "\n",
    "                serialized_role_data = row.get('Role')\n",
    "                groups = 'customer'  # Default value for groups\n",
    "                if serialized_role_data:\n",
    "                    deserialized_data = deserialize_php(serialized_role_data)\n",
    "                    if deserialized_data and any(deserialized_data.values()):\n",
    "                        groups = ', '.join([role for role, value in deserialized_data.items() if value])\n",
    "\n",
    "                new_row = {key: value for key, value in row.items() if key != 'Role'}\n",
    "                new_row['groups'] = groups\n",
    "\n",
    "                writer.writerow(new_row)\n",
    "                processed_emails.add(email)  # Add email to the processed set\n",
    "\n",
    "process_data(input_file, output_file)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:41.915389Z",
     "end_time": "2023-12-27T13:34:41.944888Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Takes new user list and changes roles to groups\n",
    "import csv\n",
    "import phpserialize\n",
    "import re\n",
    "\n",
    "input_file = 'new_users.csv'  # Update the path if needed\n",
    "output_file = 'new_roles_output.csv'\n",
    "\n",
    "def is_valid_email(email):\n",
    "    if email.startswith('**') or email.startswith('.'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def deserialize_php(serialized_php):\n",
    "    try:\n",
    "        return phpserialize.loads(serialized_php.encode(), decode_strings=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deserializing: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        # Adjust fieldnames, drop 'Role' and 'user_phone', add 'groups'\n",
    "        fieldnames = ['id' if name == 'user_id' else 'email' if name == 'user_email' else name\n",
    "                      for name in reader.fieldnames if name not in ['Role', 'user_phone']] + ['groups']\n",
    "\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                user_email = row.get('user_email')\n",
    "                if not is_valid_email(user_email):\n",
    "                    continue  # Skip rows where the email is not valid\n",
    "\n",
    "                serialized_role_data = row.get('Role')\n",
    "                groups = 'customer'  # Default value for groups\n",
    "                if serialized_role_data:\n",
    "                    deserialized_data = deserialize_php(serialized_role_data)\n",
    "                    if deserialized_data and any(deserialized_data.values()):\n",
    "                        groups = ', '.join([role for role, value in deserialized_data.items() if value])\n",
    "\n",
    "                # Create a new row dictionary without 'Role' and 'user_phone', rename 'user_id' and 'user_email'\n",
    "                new_row = {('id' if key == 'user_id' else 'email' if key == 'user_email' else key): value\n",
    "                           for key, value in row.items() if key not in ['Role', 'user_phone']}\n",
    "                new_row['groups'] = groups\n",
    "\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_data(input_file, output_file)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T14:12:28.661933Z",
     "end_time": "2023-12-27T14:12:28.686064Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Batch Version\n",
    "Used when all users are beig imported"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import csv\n",
    "import phpserialize\n",
    "import itertools\n",
    "\n",
    "input_file = 'users.csv'  # Update the path if needed\n",
    "\n",
    "def is_valid_email(email):\n",
    "    # Basic validation to check if the email doesn't start with '**' or '.'\n",
    "    if email.startswith('**') or email.startswith('.'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def deserialize_php(serialized_php):\n",
    "    try:\n",
    "        return phpserialize.loads(serialized_php.encode(), decode_strings=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deserializing: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_data(input_file, output_files):\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = [name for name in reader.fieldnames if name != 'Role'] + ['groups']\n",
    "\n",
    "        writers = []\n",
    "        outfiles = []\n",
    "        for file in output_files:\n",
    "            outfile = open(file, mode='w', newline='', encoding='utf-8')\n",
    "            outfiles.append(outfile)\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writers.append(writer)\n",
    "\n",
    "        writer_cycle = itertools.cycle(writers)\n",
    "\n",
    "        for row in reader:\n",
    "            user_email = row.get('user_email')\n",
    "            if not is_valid_email(user_email):\n",
    "                continue\n",
    "\n",
    "            serialized_role_data = row.get('Role')\n",
    "            groups = 'customer'\n",
    "            if serialized_role_data:\n",
    "                deserialized_data = deserialize_php(serialized_role_data)\n",
    "                if deserialized_data and any(deserialized_data.values()):\n",
    "                    groups = ', '.join([role for role, value in deserialized_data.items() if value])\n",
    "\n",
    "            new_row = {key: value for key, value in row.items() if key != 'Role'}\n",
    "            new_row['groups'] = groups\n",
    "\n",
    "            next(writer_cycle).writerow(new_row)\n",
    "\n",
    "    # Close all output files\n",
    "    for outfile in outfiles:\n",
    "        outfile.close()\n",
    "\n",
    "# List of output files\n",
    "output_files = ['roles_output_batch1.csv', 'roles_output_batch2.csv', 'roles_output_batch3.csv', 'roles_output_batch4.csv']\n",
    "\n",
    "process_data(input_file, output_files)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:41.962935Z",
     "end_time": "2023-12-27T13:34:41.975716Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Updated Version of batch - For all users"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import csv\n",
    "import phpserialize\n",
    "import itertools\n",
    "\n",
    "input_file = 'users.csv'  # Update the path if needed\n",
    "\n",
    "def is_valid_email(email):\n",
    "    if email.startswith('**') or email.startswith('.'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def deserialize_php(serialized_php):\n",
    "    try:\n",
    "        return phpserialize.loads(serialized_php.encode(), decode_strings=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deserializing: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_data(input_file, output_files):\n",
    "    seen_emails = set()  # Set to track already processed emails\n",
    "\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = ['id' if name == 'user_id' else ('email' if name == 'user_email' else name) for name in reader.fieldnames if name != 'Role'] + ['groups']\n",
    "\n",
    "        writers = []\n",
    "        outfiles = []\n",
    "        for file in output_files:\n",
    "            outfile = open(file, mode='w', newline='', encoding='utf-8')\n",
    "            outfiles.append(outfile)\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writers.append(writer)\n",
    "\n",
    "        writer_cycle = itertools.cycle(writers)\n",
    "\n",
    "        for row in reader:\n",
    "            user_email = row.get('user_email')\n",
    "            if not is_valid_email(user_email) or user_email in seen_emails:\n",
    "                continue  # Skip invalid emails and duplicates\n",
    "\n",
    "            seen_emails.add(user_email)  # Add email to the set of seen emails\n",
    "\n",
    "            serialized_role_data = row.get('Role')\n",
    "            groups = 'customer'\n",
    "            if serialized_role_data:\n",
    "                deserialized_data = deserialize_php(serialized_role_data)\n",
    "                if deserialized_data and any(deserialized_data.values()):\n",
    "                    groups = ', '.join([role for role, value in deserialized_data.items() if value])\n",
    "\n",
    "            new_row = {('id' if key == 'user_id' else ('email' if key == 'user_email' else key)): value for key, value in row.items() if key != 'Role'}\n",
    "            new_row['groups'] = groups\n",
    "\n",
    "            next(writer_cycle).writerow(new_row)\n",
    "\n",
    "    for outfile in outfiles:\n",
    "        outfile.close()\n",
    "\n",
    "output_files = ['roles_output_batch1.csv', 'roles_output_batch2.csv', 'roles_output_batch3.csv', 'roles_output_batch4.csv']\n",
    "process_data(input_file, output_files)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:41.981754Z",
     "end_time": "2023-12-27T13:34:41.986601Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Order Items\n",
    "Used to transpose Order Items to import into Django"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Orderitems"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def transform_csv(input_csv_path, output_csv_path):\n",
    "    with open(input_csv_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_csv_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.DictWriter(outfile, fieldnames=['id', 'type', 'quantity'])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            id = row['id']\n",
    "            for key in ['num_adults', 'num_children', 'num_senior', 'num_under3']:\n",
    "                type_name = key.split('_')[1]\n",
    "                for _ in range(int(row[key])):\n",
    "                    writer.writerow({'id': id, 'type': type_name, 'quantity': 1})\n",
    "\n",
    "# Replace 'input_csv_path' and 'output_csv_path' with your actual file paths\n",
    "input_csv_path = 'public_swims_order_items.csv'\n",
    "output_csv_path = 'public_swims_order_items_transposed.csv'\n",
    "\n",
    "transform_csv(input_csv_path, output_csv_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T13:34:41.992226Z",
     "end_time": "2023-12-27T13:34:41.998711Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Changes Order Items to the transposed version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# This works\n",
    "\n",
    "import csv\n",
    "\n",
    "def transform_csv(input_csv_path, output_csv_path):\n",
    "    with open(input_csv_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_csv_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.DictWriter(outfile, fieldnames=['id', 'variant', 'quantity'])\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Mapping from original field names to new variant names\n",
    "        variant_mappings = {\n",
    "            'num_adults': 'Adult',\n",
    "            'num_children': 'Child',\n",
    "            'num_senior': 'OAP',\n",
    "            'num_under3': 'Infant'\n",
    "        }\n",
    "\n",
    "        for row in reader:\n",
    "            id = row['id']\n",
    "            for key, new_variant_name in variant_mappings.items():\n",
    "                quantity = int(row[key])\n",
    "                if quantity > 0:\n",
    "                    writer.writerow({'id': id, 'variant': new_variant_name, 'quantity': quantity})\n",
    "\n",
    "# Replace 'input_csv_path' and 'output_csv_path' with your actual file paths\n",
    "input_csv_path = 'public_swims_order_items.csv'\n",
    "output_csv_path = 'public_swims_order_items_transposed.csv'\n",
    "\n",
    "transform_csv(input_csv_path, output_csv_path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T14:24:01.440503Z",
     "end_time": "2023-12-27T14:24:01.445907Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
